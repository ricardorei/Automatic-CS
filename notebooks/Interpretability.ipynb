{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability\n",
    "In this notebook we will explore how our models use the attention mechanism during prediction.\n",
    "\n",
    "We will only use the twitter dataset because it's the best public available corpus for a customer support domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ix =  pickle.load(open('../data/twitter/tmp/word2ix.pkl', 'rb'))\n",
    "trn_q_vecs, trn_a_vecs, trn_y = pickle.load(open('../data/twitter/tmp/de_train.pkl', 'rb'))\n",
    "ix2word = {v:k  for k,v in word2ix.items()}\n",
    "trn_q_vecs = trn_q_vecs[trn_y == 1]\n",
    "trn_a_vecs = trn_a_vecs[trn_y == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Encoders\n",
    "Lets start by loading our dual attention dual encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../retrieval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ElmoDE import ElmoDE\n",
    "from utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../data/twitter/models/transfer.ElmoDE.torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ElmoDE(\n",
       "   (encoder): SentenceEncoder(\n",
       "     (embedding): EmbeddingLayer(\n",
       "       (elmo): ElmoEmbeddings(\n",
       "         (elmo): Elmo(\n",
       "           (_elmo_lstm): _ElmoBiLm(\n",
       "             (_token_embedder): _ElmoCharacterEncoder(\n",
       "               (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "               (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "               (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "               (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "               (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "               (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "               (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "               (_highways): Highway(\n",
       "                 (_layers): ModuleList(\n",
       "                   (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "                   (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "                 )\n",
       "               )\n",
       "               (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "             )\n",
       "             (_elmo_lstm): ElmoLstm(\n",
       "               (forward_layer_0): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "               (backward_layer_0): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "               (forward_layer_1): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "               (backward_layer_1): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (_dropout): Dropout(p=0.0)\n",
       "           (scalar_mix_0): ScalarMix(\n",
       "             (scalar_parameters): ParameterList(\n",
       "                 (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "             )\n",
       "           )\n",
       "           (scalar_mix_1): ScalarMix(\n",
       "             (scalar_parameters): ParameterList(\n",
       "                 (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "             )\n",
       "           )\n",
       "           (scalar_mix_2): ScalarMix(\n",
       "             (scalar_parameters): ParameterList(\n",
       "                 (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (glove): Embedding(26470, 300, padding_idx=0)\n",
       "     )\n",
       "     (lstm): LSTM(1324, 1024, batch_first=True, bidirectional=True)\n",
       "   )\n",
       "   (dropout): Dropout(p=0.6)\n",
       "   (attn): Attention()\n",
       "   (mlp_activations): ReLU()\n",
       "   (mlp_l0_linear): Linear(in_features=6144, out_features=4096, bias=True)\n",
       "   (mlp_l1_linear): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "   (mlp_l2_linear): Linear(in_features=2048, out_features=1, bias=True)\n",
       " ), ElmoDE(\n",
       "   (encoder): SentenceEncoder(\n",
       "     (embedding): EmbeddingLayer(\n",
       "       (elmo): ElmoEmbeddings(\n",
       "         (elmo): Elmo(\n",
       "           (_elmo_lstm): _ElmoBiLm(\n",
       "             (_token_embedder): _ElmoCharacterEncoder(\n",
       "               (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "               (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "               (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "               (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "               (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "               (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "               (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "               (_highways): Highway(\n",
       "                 (_layers): ModuleList(\n",
       "                   (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "                   (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "                 )\n",
       "               )\n",
       "               (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "             )\n",
       "             (_elmo_lstm): ElmoLstm(\n",
       "               (forward_layer_0): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "               (backward_layer_0): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "               (forward_layer_1): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "               (backward_layer_1): LstmCellWithProjection(\n",
       "                 (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "                 (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "                 (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "           (_dropout): Dropout(p=0.0)\n",
       "           (scalar_mix_0): ScalarMix(\n",
       "             (scalar_parameters): ParameterList(\n",
       "                 (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "             )\n",
       "           )\n",
       "           (scalar_mix_1): ScalarMix(\n",
       "             (scalar_parameters): ParameterList(\n",
       "                 (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "             )\n",
       "           )\n",
       "           (scalar_mix_2): ScalarMix(\n",
       "             (scalar_parameters): ParameterList(\n",
       "                 (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "                 (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (glove): Embedding(26470, 300, padding_idx=0)\n",
       "     )\n",
       "     (lstm): LSTM(1324, 1024, batch_first=True, bidirectional=True)\n",
       "   )\n",
       "   (dropout): Dropout(p=0.6)\n",
       "   (attn): Attention()\n",
       "   (mlp_activations): ReLU()\n",
       "   (mlp_l0_linear): Linear(in_features=6144, out_features=4096, bias=True)\n",
       "   (mlp_l1_linear): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "   (mlp_l2_linear): Linear(in_features=2048, out_features=1, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelclass_name = model_path.split('.')[-2]\n",
    "model = getattr(sys.modules[modelclass_name], modelclass_name).load(model_path, map_location={'cuda:2':'cuda:0'})\n",
    "(model.cuda(), model.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the example:\n",
    "Now that we have imported our model we need to select a good example to plot the attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_text(q_vec, a_vec, ix):\n",
    "    pair = {}\n",
    "    pair[\"question\"] = [ix2word[i] for i in trn_q_vecs[ix]]\n",
    "    pair[\"answer\"] = [ix2word[i] for i in trn_a_vecs[ix]]\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ix = 800 # 20!! 300!! 800!! 1994!! 400!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = get_pair_text(trn_q_vecs, trn_a_vecs, example_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_APPLE_', 'I', '‚Äô', 've', 'downloaded', 'the', 'new', 'iPhone', 'update', 'and', 'my', 'battery', 'lasts', 'half', 'as', 'long', '.', 'What', '‚Äô', 's', 'going', 'on', '?', '!', '?', '_EOT_']\n"
     ]
    }
   ],
   "source": [
    "print (pair[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pair[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_USERID_', 'We', 'completely', 'understand', 'wanting', 'to', 'have', 'good', 'battery', 'life', '.', 'How', 'long', 'is', 'the', 'battery', 'lasting', 'on', 'a', 'full', 'charge', 'after', 'the', 'update', '?']\n"
     ]
    }
   ],
   "source": [
    "print (pair[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_inputs, e1_lengths, e1_idxs = pad_sequences([trn_q_vecs[example_ix]])\n",
    "e2_inputs, e2_lengths, e2_idxs = pad_sequences([trn_a_vecs[example_ix]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_attn, prob = model.get_attn_weigths(e1_inputs.cuda(), e1_lengths, e1_idxs, e2_inputs.cuda(), e2_lengths, e2_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9415]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_attention(doc, doc_attn):\n",
    "    print (len(doc),len(doc_attn))\n",
    "    attn_align = []\n",
    "    for i in range(len(doc)):\n",
    "        attn_align.append((doc[i], doc_attn[i].item()*100))\n",
    "    return attn_align, sorted(attn_align, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('üëå', 49.17473495006561),\n",
       " ('Worked', 29.57480251789093),\n",
       " ('_APPLE_', 17.98672527074814),\n",
       " ('üèΩ', 3.0652744695544243),\n",
       " ('_EOT_', 0.11834370670840144),\n",
       " ('_USERID_', 0.01909219427034259),\n",
       " ('_EOT_', 0.017554494843352586),\n",
       " ('.', 0.012524795602075756),\n",
       " ('?', 0.00774120053392835),\n",
       " ('_APPLE_', 0.004826409349334426),\n",
       " ('?', 0.004819091554963961),\n",
       " ('Please', 0.0037497171433642507),\n",
       " ('.', 0.0017171305444207974),\n",
       " ('_EOT_', 0.0014407723938347772),\n",
       " ('afterwards', 0.0012076463463017717),\n",
       " ('keep', 0.000926772099774098),\n",
       " ('_EOT_', 0.000451875575890881),\n",
       " ('.', 0.00036163633012620267),\n",
       " ('_APPLE_', 0.00025873762297123903),\n",
       " ('ima', 0.00025843701223493554),\n",
       " ('posted', 0.00024589144231867976),\n",
       " ('iOS', 0.00016307300256812596),\n",
       " ('_EOT_', 0.00015981811429810477),\n",
       " ('so', 0.00015751962791910046),\n",
       " ('update', 0.0001569897790432151),\n",
       " ('back', 0.0001437989340047352),\n",
       " ('model', 0.00013501961575457244),\n",
       " ('update', 0.00013086736316836323),\n",
       " ('Is', 0.0001254759240509884),\n",
       " ('us', 0.00012099006880816887),\n",
       " ('now', 0.0001119419152928458),\n",
       " ('it', 9.959197768694139e-05),\n",
       " ('installed', 8.480358815177169e-05),\n",
       " ('you', 5.8293380789109506e-05),\n",
       " ('you', 5.7266828434876516e-05),\n",
       " ('have', 5.5055909342627274e-05),\n",
       " ('had', 5.500583597495279e-05),\n",
       " ('11.1', 5.301687906467123e-05),\n",
       " ('Which', 5.286627242639952e-05),\n",
       " ('fuccin', 5.1963633040941204e-05),\n",
       " ('get', 4.727603766241373e-05),\n",
       " ('why', 4.695718871516874e-05),\n",
       " ('3', 4.5863214381824946e-05),\n",
       " ('11.1', 4.364269727830106e-05),\n",
       " ('important', 4.214468845020747e-05),\n",
       " ('enjoying', 4.2128775135097385e-05),\n",
       " ('do', 4.1979862430707726e-05),\n",
       " ('I', 4.129628337068425e-05),\n",
       " ('that', 3.7640373307112895e-05),\n",
       " ('says', 3.587979051644652e-05),\n",
       " ('on', 3.5291674294057884e-05),\n",
       " ('or', 3.4107617352674424e-05),\n",
       " ('No', 3.189614403709129e-05),\n",
       " ('11.0', 3.1033047775963496e-05),\n",
       " ('download', 3.09675016296751e-05),\n",
       " ('want', 2.9570907145171077e-05),\n",
       " ('go', 2.6872336889027792e-05),\n",
       " ('we', 2.6735324354376644e-05),\n",
       " ('but', 2.52120798904798e-05),\n",
       " ('do', 2.4598114123364212e-05),\n",
       " ('it', 2.3842832774789713e-05),\n",
       " ('them', 2.379898944582237e-05),\n",
       " ('ahead', 2.3184718145330407e-05),\n",
       " ('apps', 2.3139612892464356e-05),\n",
       " ('and', 1.9542937934602378e-05),\n",
       " ('_USERID_', 1.9092976799583994e-05),\n",
       " ('to', 1.4895016420268803e-05),\n",
       " ('are', 1.4819424620782229e-05),\n",
       " ('to', 1.3364009987526515e-05),\n",
       " ('won', 1.2838005147841614e-05),\n",
       " ('and', 1.0979608333627766e-05),\n",
       " ('Apps', 1.0805695183080388e-05),\n",
       " ('‚Äô', 1.0063950384164855e-05),\n",
       " ('my', 4.673049147640995e-06),\n",
       " ('t', 3.4843790075456127e-06)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_align, sorted_align = align_attention(pair[\"question\"], q_attn[0][0])\n",
    "sorted_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_align, sorted_align = align_attention(pair[\"answer\"], a_attn[0][0])\n",
    "attn_align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence\n",
    "Lets start by loading the seq2seq model.\n",
    "\n",
    "**NOTE**:\n",
    "We need to reset the notebook kernel for loading the seq2seq utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../generative')\n",
    "from seq2seq import Encoder, Decoder, Attention\n",
    "from utils import load_seq2seq, prepare_data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ix =  pickle.load(open('../data/twitter/tmp/word2ix.pkl', 'rb'))\n",
    "in_seqs, out_seqs = pickle.load(open('../data/twitter/tmp/seq2seq_test.pkl', 'rb'))\n",
    "ix2word = {v:k  for k,v in word2ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../data/twitter/models/trained.seq2seq.torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Encoder(\n",
       "   (embedding): Embedding(26470, 300)\n",
       "   (rnn): LSTM(300, 300, num_layers=2, dropout=0.2, bidirectional=True)\n",
       " ), Decoder(\n",
       "   (embedding): Embedding(26470, 300)\n",
       "   (embedding_dropout): Dropout(p=0.2)\n",
       "   (rnn): LSTM(300, 300, num_layers=2, dropout=0.2)\n",
       "   (concat): Linear(in_features=600, out_features=300, bias=True)\n",
       "   (out): Linear(in_features=300, out_features=26470, bias=True)\n",
       "   (attn): Attention()\n",
       " ), Encoder(\n",
       "   (embedding): Embedding(26470, 300)\n",
       "   (rnn): LSTM(300, 300, num_layers=2, dropout=0.2, bidirectional=True)\n",
       " ), Decoder(\n",
       "   (embedding): Embedding(26470, 300)\n",
       "   (embedding_dropout): Dropout(p=0.2)\n",
       "   (rnn): LSTM(300, 300, num_layers=2, dropout=0.2)\n",
       "   (concat): Linear(in_features=600, out_features=300, bias=True)\n",
       "   (out): Linear(in_features=300, out_features=26470, bias=True)\n",
       "   (attn): Attention()\n",
       " ))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder, decoder = load_seq2seq(model_path)\n",
    "(encoder.cuda(), decoder.cuda(), encoder.eval(), decoder.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def decoding(encoder, decoder, bos_token, input_seq, input_length, targets, mask):\n",
    "    attention_weights = []\n",
    "    with torch.no_grad():\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden, encoder_cell = encoder(input_seq, input_length)\n",
    "        # Create initial decoder input (start with BOS tokens for each sentence)\n",
    "        decoder_input = torch.LongTensor([[bos_token for _ in range(input_seq.shape[1])]]).cuda()\n",
    "        pred_tokens = decoder_input.clone()\n",
    "        # Set initial decoder hidden state to the encoder's final hidden state\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        decoder_cell = encoder_cell[:decoder.n_layers]\n",
    "        for t in range(1, targets.shape[0]):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden, decoder_cell, attn_weights = decoder.attn_forward(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "            decoder_input = targets[t].view(1, -1)\n",
    "            pred_tokens = torch.cat((pred_tokens, decoder_input.clone()), dim=0)\n",
    "            attention_weights.append(attn_weights[0][0].cpu().numpy().tolist())\n",
    "        return pred_tokens, attention_weights\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_text(q_vec, a_vec, ix):\n",
    "    pair = {}\n",
    "    pair[\"question\"] = [ix2word[i] for i in q_vec[ix]]\n",
    "    pair[\"answer\"] = [ix2word[i] for i in a_vec[ix]]\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ix = 20 #20 300 800 -- 1994!! 400!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = get_pair_text(in_seqs, out_seqs, example_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_BOS_', '_APPLE_', 'help', '!', 'My', 'phone', 'keeps', 'rebooting', 'every', 'couple', 'of', 'minutes', '.', 'This', 'screen', '_URL_', 'can', 'I', 'stop', 'this', '?', '!', '_URL_', '_EOT_', '_EOS_']\n"
     ]
    }
   ],
   "source": [
    "print (pair[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_BOS_', '_USERID_', 'We', 'know', 'the', 'importance', 'of', 'getting', 'that', 'fixed', 'quickly', '!', \"You'll\", 'want', 'to', 'get', 'your', 'device', 'updated', 'to', 'correct', 'that', 'issue', '.', \"Here's\", 'the', 'steps', ':', '_URL_', '_EOS_']\n"
     ]
    }
   ],
   "source": [
    "print (pair[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ['_BOS_', '_APPLE_', 'I', '‚Äô', 've', 'downloaded', 'the', 'new', 'iPhone', 'update', 'and', 'my', 'battery', 'lasts', 'half', 'as', 'long', '.', 'What', '‚Äô', 's', 'going', 'on', '?', '!', '?', '_EOT_', '_EOS_']\n",
    "q_in = [word2ix[word] for word in q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['_BOS_', '_USERID_', 'We', 'completely', 'understand', 'wanting', 'to', 'have', 'good', 'battery', 'life', '.', 'How', 'long', 'is', 'the', 'battery', 'lasting', 'on', 'a', 'full', 'charge', 'after', 'the', 'update', '?', '_EOS_']\n",
    "a_in = [word2ix[word] for word in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fields from batch\n",
    "inputs, lengths, targets, mask, max_target_len = prepare_data([q_in], [a_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tokens, attn_weigths = decoding(encoder, decoder, word2ix[\"_BOS_\"], inputs, lengths, targets, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_BOS_ _USERID_ We completely understand wanting to have good battery life . How long is the battery lasting on a full charge after the update ? _EOS_'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def restore_words(pred_tokens):\n",
    "    return \" \".join(ix2word[ix.item()] for ix in pred_tokens)\n",
    "\n",
    "restore_words(pred_tokens.view(1, -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_BOS_ _APPLE_ I ‚Äô ve downloaded the new iPhone update and my battery lasts half as long . What ‚Äô s going on ? ! ? _EOT_ _EOS_'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore_words(inputs.view(1, -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alignments = np.array(attn_weigths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 28)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "source_words = restore_words(inputs.view(1, -1)[0]).split()          #28\n",
    "target_words = restore_words(pred_tokens.view(1, -1)[0]).split()[1:] #26\n",
    "\n",
    "matrix = alignments\n",
    "fig, ax = plt.subplots(figsize=(25, 15))\n",
    "im = ax.imshow(matrix, cmap=\"Blues\")\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(source_words)))\n",
    "ax.set_yticks(np.arange(len(target_words)))\n",
    "\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(source_words)\n",
    "ax.set_yticklabels(target_words)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\"\"\"\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(target_words)):\n",
    "    for j in range(len(source_words)):\n",
    "        text = ax.text(j, i, int(matrix[i, j]*100), ha=\"center\", va=\"center\", color=\"black\")\n",
    "\"\"\"\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"plots/attention_align.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
